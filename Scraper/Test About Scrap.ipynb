{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-627577a3e906>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "import urllib3\n",
    "import yaml\n",
    "import utils\n",
    "import argparse\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "\n",
    "def get_facebook_images_url(img_links):\n",
    "    urls = []\n",
    "\n",
    "    for link in img_links:\n",
    "        if link != \"None\":\n",
    "            valid_url_found = False\n",
    "            driver.get(link)\n",
    "\n",
    "            try:\n",
    "                while not valid_url_found:\n",
    "                    WebDriverWait(driver, 30).until(\n",
    "                        EC.presence_of_element_located(\n",
    "                            (By.CLASS_NAME, selectors.get(\"spotlight\"))\n",
    "                        )\n",
    "                    )\n",
    "                    element = driver.find_element_by_class_name(\n",
    "                        selectors.get(\"spotlight\")\n",
    "                    )\n",
    "                    img_url = element.get_attribute(\"src\")\n",
    "\n",
    "                    if img_url.find(\".gif\") == -1:\n",
    "                        valid_url_found = True\n",
    "                        urls.append(img_url)\n",
    "            except Exception:\n",
    "                urls.append(\"None\")\n",
    "        else:\n",
    "            urls.append(\"None\")\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# takes a url and downloads image from that url\n",
    "# def image_downloader(img_links, folder_name):\n",
    "#     \"\"\"\n",
    "#     Download images from a list of image urls.\n",
    "#     :param img_links:\n",
    "#     :param folder_name:\n",
    "#     :return: list of image names downloaded\n",
    "#     \"\"\"\n",
    "#     img_names = []\n",
    "\n",
    "#     try:\n",
    "#         parent = os.getcwd()\n",
    "#         try:\n",
    "#             folder = os.path.join(os.getcwd(), folder_name)\n",
    "#             utils.create_folder(folder)\n",
    "#             os.chdir(folder)\n",
    "#         except Exception:\n",
    "#             print(\"Error in changing directory.\")\n",
    "\n",
    "#         for link in img_links:\n",
    "#             img_name = \"None\"\n",
    "\n",
    "#             if link != \"None\":\n",
    "#                 img_name = (link.split(\".jpg\")[0]).split(\"/\")[-1] + \".jpg\"\n",
    "\n",
    "#                 # this is the image id when there's no profile pic\n",
    "#                 if img_name == selectors.get(\"default_image\"):\n",
    "#                     img_name = \"None\"\n",
    "#                 else:\n",
    "#                     try:\n",
    "#                         urllib.request.urlretrieve(link, img_name)\n",
    "#                     except Exception:\n",
    "#                         img_name = \"None\"\n",
    "\n",
    "#             img_names.append(img_name)\n",
    "\n",
    "#         os.chdir(parent)\n",
    "#     except Exception:\n",
    "#         print(\"Exception (image_downloader):\", sys.exc_info()[0])\n",
    "#     return img_names\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "def extract_and_write_posts(elements, filename):\n",
    "    try:\n",
    "        f = open(filename, \"w\", newline=\"\\r\\n\", encoding=\"utf-8\")\n",
    "        f.writelines(\n",
    "            \" TIME || TYPE  || TITLE || STATUS  ||   LINKS(Shared Posts/Shared Links etc) || POST_ID \"\n",
    "            + \"\\n\"\n",
    "            + \"\\n\"\n",
    "        )\n",
    "        ids = []\n",
    "        for x in elements:\n",
    "            try:\n",
    "                link = \"\"\n",
    "                # id\n",
    "                post_id = utils.get_post_id(x)\n",
    "                ids.append(post_id)\n",
    "\n",
    "                # time\n",
    "                time = utils.get_time(x)\n",
    "\n",
    "                link, status, title, post_type = get_status_and_title(link, x)\n",
    "\n",
    "                line = (\n",
    "                    str(time)\n",
    "                    + \" || \"\n",
    "                    + str(post_type)\n",
    "                    + \" || \"\n",
    "                    + str(title)\n",
    "                    + \" || \"\n",
    "                    + str(status)\n",
    "                    + \" || \"\n",
    "                    + str(link)\n",
    "                    + \" || \"\n",
    "                    + str(post_id)\n",
    "                    + \"\\n\"\n",
    "                )\n",
    "\n",
    "                try:\n",
    "                    f.writelines(line)\n",
    "                except Exception:\n",
    "                    print(\"Posts: Could not map encoded characters\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        f.close()\n",
    "    except ValueError:\n",
    "        print(\"Exception (extract_and_write_posts)\",\n",
    "              \"Status =\", sys.exc_info()[0])\n",
    "    except Exception:\n",
    "        print(\"Exception (extract_and_write_posts)\",\n",
    "              \"Status =\", sys.exc_info()[0])\n",
    "    return\n",
    "\n",
    "\n",
    "def get_status_and_title(link, x):\n",
    "    # title\n",
    "    title = utils.get_title(x, selectors)\n",
    "    if title.text.find(\"shared a memory\") != -1:\n",
    "        x = x.find_element_by_xpath(selectors.get(\"title_element\"))\n",
    "        title = utils.get_title(x, selectors)\n",
    "    status = utils.get_status(x, selectors)\n",
    "    if title.text == driver.find_element_by_id(selectors.get(\"title_text\")).text:\n",
    "        if status == \"\":\n",
    "            temp = utils.get_div_links(x, \"img\", selectors)\n",
    "            if temp == \"\":  # no image tag which means . it is not a life event\n",
    "                link = utils.get_div_links(\n",
    "                    x, \"a\", selectors).get_attribute(\"href\")\n",
    "                post_type = \"status update without text\"\n",
    "            else:\n",
    "                post_type = \"life event\"\n",
    "                link = utils.get_div_links(\n",
    "                    x, \"a\", selectors).get_attribute(\"href\")\n",
    "                status = utils.get_div_links(x, \"a\", selectors).text\n",
    "        else:\n",
    "            post_type = \"status update\"\n",
    "            if utils.get_div_links(x, \"a\", selectors) != \"\":\n",
    "                link = utils.get_div_links(\n",
    "                    x, \"a\", selectors).get_attribute(\"href\")\n",
    "\n",
    "    elif title.text.find(\" shared \") != -1:\n",
    "        x1, link = utils.get_title_links(title)\n",
    "        post_type = \"shared \" + x1\n",
    "    elif title.text.find(\" at \") != -1 or title.text.find(\" in \") != -1:\n",
    "        if title.text.find(\" at \") != -1:\n",
    "            x1, link = utils.get_title_links(title)\n",
    "            post_type = \"check in\"\n",
    "        elif title.text.find(\" in \") != 1:\n",
    "            status = utils.get_div_links(x, \"a\", selectors).text\n",
    "    elif title.text.find(\" added \") != -1 and title.text.find(\"photo\") != -1:\n",
    "        post_type = \"added photo\"\n",
    "        link = utils.get_div_links(x, \"a\", selectors).get_attribute(\"href\")\n",
    "\n",
    "    elif title.text.find(\" added \") != -1 and title.text.find(\"video\") != -1:\n",
    "        post_type = \"added video\"\n",
    "        link = utils.get_div_links(x, \"a\", selectors).get_attribute(\"href\")\n",
    "\n",
    "    else:\n",
    "        post_type = \"others\"\n",
    "    if not isinstance(title, str):\n",
    "        title = title.text\n",
    "    status = status.replace(\"\\n\", \" \")\n",
    "    title = title.replace(\"\\n\", \" \")\n",
    "    return link, status, title, post_type\n",
    "\n",
    "\n",
    "def extract_and_write_group_posts(elements, filename):\n",
    "    try:\n",
    "        f = create_post_file(filename)\n",
    "        ids = []\n",
    "        for x in elements:\n",
    "            try:\n",
    "                # id\n",
    "                post_id = utils.get_group_post_id(x)\n",
    "                ids.append(post_id)\n",
    "            except Exception:\n",
    "                pass\n",
    "        total = len(ids)\n",
    "        i = 0\n",
    "        for post_id in ids:\n",
    "            i += 1\n",
    "            try:\n",
    "                add_group_post_to_file(\n",
    "                    f, filename, post_id, i, total, reload=True)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        f.close()\n",
    "    except ValueError:\n",
    "        print(\"Exception (extract_and_write_posts)\",\n",
    "              \"Status =\", sys.exc_info()[0])\n",
    "    except Exception:\n",
    "        print(\"Exception (extract_and_write_posts)\",\n",
    "              \"Status =\", sys.exc_info()[0])\n",
    "    return\n",
    "\n",
    "\n",
    "def add_group_post_to_file(f, filename, post_id, number=1, total=1, reload=False):\n",
    "    print(\"Scraping Post(\" + post_id + \"). \" +\n",
    "          str(number) + \" of \" + str(total))\n",
    "    photos_dir = os.path.dirname(filename)\n",
    "    if reload:\n",
    "        driver.get(utils.create_post_link(post_id, selectors))\n",
    "    line = get_group_post_as_line(post_id, photos_dir)\n",
    "    try:\n",
    "        f.writelines(line)\n",
    "    except Exception:\n",
    "        print(\"Posts: Could not map encoded characters\")\n",
    "\n",
    "\n",
    "def create_post_file(filename):\n",
    "    \"\"\"\n",
    "    Creates post file and header\n",
    "    :param filename:\n",
    "    :return: file\n",
    "    \"\"\"\n",
    "    f = open(filename, \"w\", newline=\"\\r\\n\", encoding=\"utf-8\")\n",
    "    f.writelines(\n",
    "        \"TIME || TYPE  || TITLE || STATUS || LINKS(Shared Posts/Shared Links etc) || POST_ID || \"\n",
    "        \"PHOTO || COMMENTS \" + \"\\n\"\n",
    "    )\n",
    "    return f\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "\n",
    "def save_to_file(name, elements, status, current_section):\n",
    "    \"\"\"helper function used to save links to files\"\"\"\n",
    "\n",
    "    # status 0 = dealing with friends list\n",
    "    # status 1 = dealing with photos\n",
    "    # status 2 = dealing with videos\n",
    "    # status 3 = dealing with about section\n",
    "    # status 4 = dealing with posts\n",
    "    # status 5 = dealing with group posts\n",
    "\n",
    "    try:\n",
    "        f = None  # file pointer\n",
    "\n",
    "        if status != 4 and status != 5:\n",
    "            f = open(name, \"w\", encoding=\"utf-8\", newline=\"\\r\\n\")\n",
    "\n",
    "        results = []\n",
    "        img_names = []\n",
    "\n",
    "        # dealing with Friends\n",
    "        if status == 0:\n",
    "            # get profile links of friends\n",
    "            results = [x.get_attribute(\"href\") for x in elements]\n",
    "            results = [create_original_link(x) for x in results]\n",
    "\n",
    "            # get names of friends\n",
    "            people_names = [\n",
    "                x.find_element_by_tag_name(\"img\").get_attribute(\"aria-label\")\n",
    "                for x in elements\n",
    "            ]\n",
    "\n",
    "            # download friends' photos\n",
    "            try:\n",
    "                if download_friends_photos:\n",
    "                    if friends_small_size:\n",
    "                        img_links = [\n",
    "                            x.find_element_by_css_selector(\n",
    "                                \"img\").get_attribute(\"src\")\n",
    "                            for x in elements\n",
    "                        ]\n",
    "                    else:\n",
    "                        links = []\n",
    "                        for friend in results:\n",
    "                            try:\n",
    "                                driver.get(friend)\n",
    "                                WebDriverWait(driver, 30).until(\n",
    "                                    EC.presence_of_element_located(\n",
    "                                        (\n",
    "                                            By.CLASS_NAME,\n",
    "                                            selectors.get(\"profilePicThumb\"),\n",
    "                                        )\n",
    "                                    )\n",
    "                                )\n",
    "                                l = driver.find_element_by_class_name(\n",
    "                                    selectors.get(\"profilePicThumb\")\n",
    "                                ).get_attribute(\"href\")\n",
    "                            except Exception:\n",
    "                                l = \"None\"\n",
    "\n",
    "                            links.append(l)\n",
    "\n",
    "                        for i, _ in enumerate(links):\n",
    "                            if links[i] is None:\n",
    "                                links[i] = \"None\"\n",
    "                            elif links[i].find(\"picture/view\") != -1:\n",
    "                                links[i] = \"None\"\n",
    "\n",
    "                        img_links = get_facebook_images_url(links)\n",
    "\n",
    "                    folder_names = [\n",
    "                        \"Friend's Photos\",\n",
    "                        \"Mutual Friends' Photos\",\n",
    "                        \"Following's Photos\",\n",
    "                        \"Follower's Photos\",\n",
    "                        \"Work Friends Photos\",\n",
    "                        \"College Friends Photos\",\n",
    "                        \"Current City Friends Photos\",\n",
    "                        \"Hometown Friends Photos\",\n",
    "                    ]\n",
    "                    print(\"Downloading \" + folder_names[current_section])\n",
    "\n",
    "                    img_names = image_downloader(\n",
    "                        img_links, folder_names[current_section]\n",
    "                    )\n",
    "                else:\n",
    "                    img_names = [\"None\"] * len(results)\n",
    "            except Exception:\n",
    "                print(\n",
    "                    \"Exception (Images)\",\n",
    "                    str(status),\n",
    "                    \"Status =\",\n",
    "                    current_section,\n",
    "                    sys.exc_info()[0],\n",
    "                )\n",
    "\n",
    "        # dealing with Photos\n",
    "        # elif status == 1:\n",
    "        #     results = [x.get_attribute(\"href\") for x in elements]\n",
    "        #     results.pop(0)\n",
    "\n",
    "        #     try:\n",
    "        #         if download_uploaded_photos:\n",
    "        #             if photos_small_size:\n",
    "        #                 background_img_links = driver.find_elements_by_xpath(\n",
    "        #                     selectors.get(\"background_img_links\")\n",
    "        #                 )\n",
    "        #                 background_img_links = [\n",
    "        #                     x.get_attribute(\"style\") for x in background_img_links\n",
    "        #                 ]\n",
    "        #                 background_img_links = [\n",
    "        #                     ((x.split(\"(\")[1]).split(\")\")[0]).strip('\"')\n",
    "        #                     for x in background_img_links\n",
    "        #                 ]\n",
    "        #             else:\n",
    "        #                 background_img_links = get_facebook_images_url(results)\n",
    "\n",
    "        #             folder_names = [\"Uploaded Photos\", \"Tagged Photos\"]\n",
    "        #             print(\"Downloading \" + folder_names[current_section])\n",
    "\n",
    "        #             img_names = image_downloader(\n",
    "        #                 background_img_links, folder_names[current_section]\n",
    "        #             )\n",
    "        #         else:\n",
    "        #             img_names = [\"None\"] * len(results)\n",
    "        #     except Exception:\n",
    "        #         print(\n",
    "        #             \"Exception (Images)\",\n",
    "        #             str(status),\n",
    "        #             \"Status =\",\n",
    "        #             current_section,\n",
    "        #             sys.exc_info()[0],\n",
    "        #         )\n",
    "\n",
    "        # dealing with Videos\n",
    "        # elif status == 2:\n",
    "        #     results = elements[0].find_elements_by_css_selector(\"li\")\n",
    "        #     results = [\n",
    "        #         x.find_element_by_css_selector(\"a\").get_attribute(\"href\")\n",
    "        #         for x in results\n",
    "        #     ]\n",
    "\n",
    "        #     try:\n",
    "        #         if results[0][0] == \"/\":\n",
    "        #             results = [r.pop(0) for r in results]\n",
    "        #             results = [(selectors.get(\"fb_link\") + x) for x in results]\n",
    "        #     except Exception:\n",
    "        #         pass\n",
    "\n",
    "        # dealing with About Section\n",
    "        elif status == 3:\n",
    "            results = elements[0].text\n",
    "            f.writelines(results)\n",
    "\n",
    "        # dealing with Posts\n",
    "        # elif status == 4:\n",
    "        #     extract_and_write_posts(elements, name)\n",
    "        #     return\n",
    "\n",
    "        # # dealing with Group Posts\n",
    "        # elif status == 5:\n",
    "        #     extract_and_write_group_posts(elements, name)\n",
    "        #     return\n",
    "\n",
    "        \"\"\"Write results to file\"\"\"\n",
    "        # if status == 0:\n",
    "        #     for i, _ in enumerate(results):\n",
    "        #         # friend's profile link\n",
    "        #         f.writelines(results[i])\n",
    "        #         f.write(\",\")\n",
    "\n",
    "        #         # friend's name\n",
    "        #         f.writelines(people_names[i])\n",
    "        #         f.write(\",\")\n",
    "\n",
    "        #         # friend's downloaded picture id\n",
    "        #         f.writelines(img_names[i])\n",
    "        #         f.write(\"\\n\")\n",
    "\n",
    "        # elif status == 1:\n",
    "        #     for i, _ in enumerate(results):\n",
    "        #         # image's link\n",
    "        #         f.writelines(results[i])\n",
    "        #         f.write(\",\")\n",
    "\n",
    "        #         # downloaded picture id\n",
    "        #         f.writelines(img_names[i])\n",
    "        #         f.write(\"\\n\")\n",
    "\n",
    "        # elif status == 2:\n",
    "        #     for x in results:\n",
    "        #         f.writelines(x + \"\\n\")\n",
    "\n",
    "        f.close()\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Exception (save_to_file)\", \"Status =\",\n",
    "              str(status), sys.exc_info()[0])\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def scrape_data(url, scan_list, section, elements_path, save_status, file_names):\n",
    "    \"\"\"Given some parameters, this function can scrap friends/photos/videos/about/posts(statuses) of a profile\"\"\"\n",
    "    page = []\n",
    "\n",
    "    if save_status == 4 or save_status == 5:\n",
    "        page.append(url)\n",
    "\n",
    "    page += [url + s for s in section]\n",
    "\n",
    "    for i, _ in enumerate(scan_list):\n",
    "        try:\n",
    "            driver.get(page[i])\n",
    "\n",
    "            if (\n",
    "                (save_status == 0) or (save_status == 1) or (save_status == 2)\n",
    "            ):  # Only run this for friends, photos and videos\n",
    "\n",
    "                # the bar which contains all the sections\n",
    "                sections_bar = driver.find_element_by_xpath(\n",
    "                    selectors.get(\"sections_bar\")\n",
    "                )\n",
    "\n",
    "                if sections_bar.text.find(scan_list[i]) == -1:\n",
    "                    continue\n",
    "\n",
    "            if save_status != 3:\n",
    "                utils.scroll(total_scrolls, driver, selectors, scroll_time)\n",
    "                pass\n",
    "\n",
    "            data = driver.find_elements_by_xpath(elements_path[i])\n",
    "\n",
    "            save_to_file(file_names[i], data, save_status, i)\n",
    "\n",
    "        except Exception:\n",
    "            print(\n",
    "                \"Exception (scrape_data)\",\n",
    "                str(i),\n",
    "                \"Status =\",\n",
    "                str(save_status),\n",
    "                sys.exc_info()[0],\n",
    "            )\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def create_original_link(url):\n",
    "    if url.find(\".php\") != -1:\n",
    "        original_link = (\n",
    "            facebook_https_prefix + facebook_link_body + ((url.split(\"=\"))[1])\n",
    "        )\n",
    "\n",
    "        if original_link.find(\"&\") != -1:\n",
    "            original_link = original_link.split(\"&\")[0]\n",
    "\n",
    "    elif url.find(\"fnr_t\") != -1:\n",
    "        original_link = (\n",
    "            facebook_https_prefix\n",
    "            + facebook_link_body\n",
    "            + ((url.split(\"/\"))[-1].split(\"?\")[0])\n",
    "        )\n",
    "    elif url.find(\"_tab\") != -1:\n",
    "        original_link = (\n",
    "            facebook_https_prefix\n",
    "            + facebook_link_body\n",
    "            + (url.split(\"?\")[0]).split(\"/\")[-1]\n",
    "        )\n",
    "    else:\n",
    "        original_link = url\n",
    "\n",
    "    return original_link\n",
    "\n",
    "\n",
    "def scrap_profile():\n",
    "    data_folder = os.path.join(os.getcwd(), \"data\")\n",
    "    utils.create_folder(data_folder)\n",
    "    os.chdir(data_folder)\n",
    "\n",
    "    # execute for all profiles given in input.txt file\n",
    "    url = driver.current_url\n",
    "    user_id = create_original_link(url)\n",
    "\n",
    "    print(\"\\nScraping:\", user_id)\n",
    "\n",
    "    try:\n",
    "        target_dir = os.path.join(data_folder, user_id.split(\"/\")[-1])\n",
    "        utils.create_folder(target_dir)\n",
    "        os.chdir(target_dir)\n",
    "    except Exception:\n",
    "        print(\"Some error occurred in creating the profile directory.\")\n",
    "        os.chdir(\"../..\")\n",
    "        return\n",
    "\n",
    "    to_scrap = [\"About\"]\n",
    "    for item in to_scrap:\n",
    "        print(\"----------------------------------------\")\n",
    "        print(\"Scraping {}..\".format(item))\n",
    "\n",
    "        if item == \"About\":\n",
    "            scan_list = [None] * 7\n",
    "        else:\n",
    "            scan_list = params[item][\"scan_list\"]\n",
    "\n",
    "        section = params[item][\"section\"]\n",
    "        elements_path = params[item][\"elements_path\"]\n",
    "        file_names = params[item][\"file_names\"]\n",
    "        save_status = params[item][\"save_status\"]\n",
    "\n",
    "        scrape_data(user_id, scan_list, section,\n",
    "                    elements_path, save_status, file_names)\n",
    "\n",
    "        print(\"{} Done!\".format(item))\n",
    "\n",
    "    print(\"Finished Scraping Profile \" + str(user_id) + \".\")\n",
    "    os.chdir(\"../..\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "# def get_comments():\n",
    "#     comments = []\n",
    "#     try:\n",
    "#         data = driver.find_element_by_xpath(selectors.get(\"comment_section\"))\n",
    "#         reply_links = driver.find_elements_by_xpath(\n",
    "#             selectors.get(\"more_comment_replies\")\n",
    "#         )\n",
    "#         for link in reply_links:\n",
    "#             try:\n",
    "#                 driver.execute_script(\"arguments[0].click();\", link)\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "#         see_more_links = driver.find_elements_by_xpath(\n",
    "#             selectors.get(\"comment_see_more_link\")\n",
    "#         )\n",
    "#         for link in see_more_links:\n",
    "#             try:\n",
    "#                 driver.execute_script(\"arguments[0].click();\", link)\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "#         data = data.find_elements_by_xpath(selectors.get(\"comment\"))\n",
    "#         for d in data:\n",
    "#             try:\n",
    "#                 author = d.find_element_by_xpath(\n",
    "#                     selectors.get(\"comment_author\")).text\n",
    "#                 text = d.find_element_by_xpath(\n",
    "#                     selectors.get(\"comment_text\")).text\n",
    "#                 replies = utils.get_replies(d, selectors)\n",
    "#                 comments.append([author, text, replies])\n",
    "#             except Exception:\n",
    "#                 pass\n",
    "#     except Exception:\n",
    "#         pass\n",
    "#     return comments\n",
    "\n",
    "\n",
    "# def get_group_post_as_line(post_id, photos_dir):\n",
    "#     try:\n",
    "#         data = driver.find_element_by_xpath(selectors.get(\"single_post\"))\n",
    "#         time = utils.get_time(data)\n",
    "#         title = utils.get_title(data, selectors).text\n",
    "#         # link, status, title, type = get_status_and_title(title,data)\n",
    "#         link = utils.get_div_links(data, \"a\", selectors)\n",
    "#         if link != \"\":\n",
    "#             link = link.get_attribute(\"href\")\n",
    "#         post_type = \"\"\n",
    "#         status = '\"' + \\\n",
    "#             utils.get_status(data, selectors).replace(\"\\r\\n\", \" \") + '\"'\n",
    "#         photos = utils.get_post_photos_links(\n",
    "#             data, selectors, photos_small_size)\n",
    "#         comments = get_comments()\n",
    "#         photos = image_downloader(photos, photos_dir)\n",
    "#         line = (\n",
    "#             str(time)\n",
    "#             + \"||\"\n",
    "#             + str(post_type)\n",
    "#             + \"||\"\n",
    "#             + str(title)\n",
    "#             + \"||\"\n",
    "#             + str(status)\n",
    "#             + \"||\"\n",
    "#             + str(link)\n",
    "#             + \"||\"\n",
    "#             + str(post_id)\n",
    "#             + \"||\"\n",
    "#             + str(photos)\n",
    "#             + \"||\"\n",
    "#             + str(comments)\n",
    "#             + \"\\n\"\n",
    "#         )\n",
    "#         return line\n",
    "#     except Exception:\n",
    "#         return \"\"\n",
    "\n",
    "\n",
    "def create_folders():\n",
    "    \"\"\"\n",
    "    Creates folder for saving data (profile, post or group) according to current driver url\n",
    "    Changes current dir to target_dir\n",
    "    :return: target_dir or None in case of failure\n",
    "    \"\"\"\n",
    "    folder = os.path.join(os.getcwd(), \"data\")\n",
    "    utils.create_folder(folder)\n",
    "    os.chdir(folder)\n",
    "    try:\n",
    "        item_id = get_item_id(driver.current_url)\n",
    "        target_dir = os.path.join(folder, item_id)\n",
    "        utils.create_folder(target_dir)\n",
    "        os.chdir(target_dir)\n",
    "        return target_dir\n",
    "    except Exception:\n",
    "        print(\"Some error occurred in creating the group directory.\")\n",
    "        os.chdir(\"../..\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_item_id(url):\n",
    "    \"\"\"\n",
    "    Gets item id from url\n",
    "    :param url: facebook url string\n",
    "    :return: item id or empty string in case of failure\n",
    "    \"\"\"\n",
    "    ret = \"\"\n",
    "    try:\n",
    "        link = create_original_link(url)\n",
    "        ret = link.split(\"/\")[-1]\n",
    "        if ret.strip() == \"\":\n",
    "            ret = link.split(\"/\")[-2]\n",
    "    except Exception as e:\n",
    "        print(\"Failed to get id: \" + format(e))\n",
    "    return ret\n",
    "\n",
    "\n",
    "# def scrape_group(url):\n",
    "#     if create_folders() is None:\n",
    "#         return\n",
    "#     group_id = get_item_id(url)\n",
    "#     # execute for all profiles given in input.txt file\n",
    "#     print(\"\\nScraping:\", group_id)\n",
    "\n",
    "#     to_scrap = [\"GroupPosts\"]  # , \"Photos\", \"Videos\", \"About\"]\n",
    "#     for item in to_scrap:\n",
    "#         print(\"----------------------------------------\")\n",
    "#         print(\"Scraping {}..\".format(item))\n",
    "\n",
    "#         if item == \"GroupPosts\":\n",
    "#             scan_list = [None]\n",
    "#         elif item == \"About\":\n",
    "#             scan_list = [None] * 7\n",
    "#         else:\n",
    "#             scan_list = params[item][\"scan_list\"]\n",
    "\n",
    "#         section = params[item][\"section\"]\n",
    "#         elements_path = params[item][\"elements_path\"]\n",
    "#         file_names = params[item][\"file_names\"]\n",
    "#         save_status = params[item][\"save_status\"]\n",
    "\n",
    "#         scrape_data(url, scan_list, section,\n",
    "#                     elements_path, save_status, file_names)\n",
    "\n",
    "#         print(\"{} Done!\".format(item))\n",
    "\n",
    "#     print(\"Finished Scraping Group \" + str(group_id) + \".\")\n",
    "#     os.chdir(\"../..\")\n",
    "\n",
    "#     return\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def login(email, password):\n",
    "    \"\"\" Logging into our own profile \"\"\"\n",
    "\n",
    "    try:\n",
    "        global driver\n",
    "\n",
    "        options = Options()\n",
    "\n",
    "        #  Code to disable notifications pop up of Chrome Browser\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--disable-infobars\")\n",
    "        options.add_argument(\"--mute-audio\")\n",
    "        # options.add_argument(\"headless\")\n",
    "\n",
    "        try:\n",
    "            driver = webdriver.Chrome(\n",
    "                executable_path=ChromeDriverManager().install(), options=options\n",
    "            )\n",
    "        except Exception:\n",
    "            print(\"Error loading chrome webdriver \" + sys.exc_info()[0])\n",
    "            exit(1)\n",
    "\n",
    "        fb_path = facebook_https_prefix + facebook_link_body\n",
    "        driver.get(fb_path)\n",
    "        driver.maximize_window()\n",
    "\n",
    "        # filling the form\n",
    "        driver.find_element_by_name(\"email\").send_keys(email)\n",
    "        driver.find_element_by_name(\"pass\").send_keys(password)\n",
    "\n",
    "        try:\n",
    "            # clicking on login button\n",
    "            driver.find_element_by_id(\"loginbutton\").click()\n",
    "        except NoSuchElementException:\n",
    "            # Facebook new design\n",
    "            driver.find_element_by_name(\"login\").click()\n",
    "\n",
    "        # if your account uses multi factor authentication\n",
    "        mfa_code_input = utils.safe_find_element_by_id(\n",
    "            driver, \"approvals_code\")\n",
    "\n",
    "        if mfa_code_input is None:\n",
    "            return\n",
    "\n",
    "        mfa_code_input.send_keys(input(\"Enter MFA code: \"))\n",
    "        driver.find_element_by_id(\"checkpointSubmitButton\").click()\n",
    "\n",
    "        # there are so many screens asking you to verify things. Just skip them all\n",
    "        while (\n",
    "            utils.safe_find_element_by_id(\n",
    "                driver, \"checkpointSubmitButton\") is not None\n",
    "        ):\n",
    "            dont_save_browser_radio = utils.safe_find_element_by_id(\n",
    "                driver, \"u_0_3\")\n",
    "            if dont_save_browser_radio is not None:\n",
    "                dont_save_browser_radio.click()\n",
    "\n",
    "            driver.find_element_by_id(\"checkpointSubmitButton\").click()\n",
    "\n",
    "    except Exception:\n",
    "        print(\"There's some error in log in.\")\n",
    "        print(sys.exc_info()[0])\n",
    "        exit(1)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def scraper(**kwargs):\n",
    "    with open(\"credentials.yaml\", \"r\") as ymlfile:\n",
    "        cfg = yaml.safe_load(stream=ymlfile)\n",
    "\n",
    "    if (\"password\" not in cfg) or (\"email\" not in cfg):\n",
    "        print(\"Your email or password is missing. Kindly write them in credentials.txt\")\n",
    "        exit(1)\n",
    "    urls = [\n",
    "        facebook_https_prefix + facebook_link_body + get_item_id(line)\n",
    "        for line in open(\"input.txt\", newline=\"\\r\\n\")\n",
    "        if not line.lstrip().startswith(\"#\") and not line.strip() == \"\"\n",
    "    ]\n",
    "\n",
    "    if len(urls) > 0:\n",
    "        print(\"\\nStarting Scraping...\")\n",
    "        login(cfg[\"email\"], cfg[\"password\"])\n",
    "        for url in urls:\n",
    "            driver.get(url)\n",
    "            link_type = utils.identify_url(driver.current_url)\n",
    "            if link_type == 0:\n",
    "                scrap_profile()\n",
    "            elif link_type == 1:\n",
    "                # scrap_post(url)\n",
    "                pass\n",
    "            elif link_type == 2:\n",
    "                # scrape_group(driver.current_url)\n",
    "                pass\n",
    "            elif link_type == 3:\n",
    "                file_name = params[\"GroupPosts\"][\"file_names\"][0]\n",
    "                item_id = get_item_id(driver.current_url)\n",
    "                if create_folders() is None:\n",
    "                    continue\n",
    "                f = create_post_file(file_name)\n",
    "                add_group_post_to_file(f, file_name, item_id)\n",
    "                f.close()\n",
    "                os.chdir(\"../..\")\n",
    "        driver.close()\n",
    "    else:\n",
    "        print(\"Input file is empty.\")\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ap = argparse.ArgumentParser()\n",
    "    # PLS CHECK IF HELP CAN BE BETTER / LESS AMBIGUOUS\n",
    "    ap.add_argument(\n",
    "        \"-dup\",\n",
    "        \"--uploaded_photos\",\n",
    "        help=\"download users' uploaded photos?\",\n",
    "        default=True,\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"-dfp\", \"--friends_photos\", help=\"download users' photos?\", default=True\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"-fss\",\n",
    "        \"--friends_small_size\",\n",
    "        help=\"Download friends pictures in small size?\",\n",
    "        default=True,\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"-pss\",\n",
    "        \"--photos_small_size\",\n",
    "        help=\"Download photos in small size?\",\n",
    "        default=True,\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"-ts\",\n",
    "        \"--total_scrolls\",\n",
    "        help=\"How many times should I scroll down?\",\n",
    "        default=2500,\n",
    "    )\n",
    "    ap.add_argument(\n",
    "        \"-st\", \"--scroll_time\", help=\"How much time should I take to scroll?\", default=8\n",
    "    )\n",
    "\n",
    "    args = vars(ap.parse_args())\n",
    "    print(args)\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # Global Variables\n",
    "    # ---------------------------------------------------------\n",
    "\n",
    "    # whether to download photos or not\n",
    "    # download_uploaded_photos = utils.to_bool(args[\"uploaded_photos\"])\n",
    "    # download_friends_photos = utils.to_bool(args[\"friends_photos\"])\n",
    "\n",
    "    # whether to download the full image or its thumbnail (small size)\n",
    "    # if small size is True then it will be very quick else if its false then it will open each photo to download it\n",
    "    # and it will take much more time\n",
    "    # friends_small_size = utils.to_bool(args[\"friends_small_size\"])\n",
    "    # photos_small_size = utils.to_bool(args[\"photos_small_size\"])\n",
    "\n",
    "    total_scrolls = int(args[\"total_scrolls\"])\n",
    "    scroll_time = int(args[\"scroll_time\"])\n",
    "\n",
    "    current_scrolls = 0\n",
    "    old_height = 0\n",
    "\n",
    "    driver = None\n",
    "\n",
    "    with open(\"selectors.json\") as a, open(\"params.json\") as b:\n",
    "        selectors = json.load(a)\n",
    "        params = json.load(b)\n",
    "\n",
    "    firefox_profile_path = selectors.get(\"firefox_profile_path\")\n",
    "    facebook_https_prefix = selectors.get(\"facebook_https_prefix\")\n",
    "    facebook_link_body = selectors.get(\"facebook_link_body\")\n",
    "\n",
    "    # get things rolling\n",
    "    scraper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
